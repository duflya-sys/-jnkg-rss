import requests
import json
import pandas as pd
import os
from datetime import datetime, timedelta
import time
import logging
import sys

# é…ç½®æ—¥å¿— - ä¿®å¤è¯­æ³•é”™è¯¯
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('bidding_crawler.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

class JnkgBiddingSpider:
    def __init__(self):
        self.base_url = "https://dzzb.jnkgjtdzzbgs.com"
        self.api_url = f"{self.base_url}/cms/api/dynamicData/queryContentPage"
        
        # å®šä¹‰å¤šä¸ªç½‘ç«™çš„é…ç½®
        self.website_configs = [
            {
                "name": "3ywgg1",
                "url": "/cms/default/webfile/3ywgg1/index.html",
                "site_id": "725",
                "category_id": "238"
            },
            {
                "name": "2ywgg1", 
                "url": "/cms/default/webfile/2ywgg1/index.html",
                "site_id": "725",  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                "category_id": "230"  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            },
            {
                "name": "1ywgg1",
                "url": "/cms/default/webfile/1ywgg1/index.html",
                "site_id": "725",  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
                "category_id": "222"  # éœ€è¦æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
            }
        ]
        
        # å¦‚æœä¸çŸ¥é“å…¶ä»–ç½‘ç«™çš„site_idå’Œcategory_idï¼Œå¯ä»¥å…ˆä½¿ç”¨é»˜è®¤å€¼
        # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ªç½‘ç«™çš„é…ç½®ä½œä¸ºfallback
        self.default_site_id = self.website_configs[0]["site_id"]
        self.default_category_id = self.website_configs[0]["category_id"]
        
        self.page_size = 20
        
        # æœç´¢å…³é”®è¯
        self.keywords = ["å¤©å®‰","æ™‹åœ£","æ™‹ç…¤"]
        
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Content-Type': 'application/json; charset=utf-8',
            'Origin': self.base_url,
        }
        
        # ============ã€åœ¨æ­¤å¤„æ·»åŠ ä»£ç†é…ç½®ã€‘============
        # ä»£ç†é…ç½®
        self.proxy_config = {
            'http': 'http://117.69.236.166:8089',
            'https': 'http://117.69.236.166:8089'
        }
        
        # æ£€æŸ¥æ˜¯å¦åœ¨GitHub Actionsç¯å¢ƒ
        self.is_github_actions = os.getenv('GITHUB_ACTIONS') == 'true'
        self.use_proxy = self.is_github_actions  # åœ¨GitHub Actionsä¸­è‡ªåŠ¨ä½¿ç”¨ä»£ç†
        
        if self.use_proxy:
            print("ğŸŒ æ£€æµ‹åˆ°GitHub Actionsç¯å¢ƒï¼Œå¯ç”¨ä»£ç†")
            print(f"ğŸ”— ä»£ç†åœ°å€: {self.proxy_config['http']}")
        # ============ã€ä»£ç†é…ç½®ç»“æŸã€‘============
        
        # æ˜¾ç¤ºå½“å‰å·¥ä½œç›®å½•
        print(f"ğŸ“‚ å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}")
        print(f"ğŸ“‚ è¾“å‡ºæ–‡ä»¶å°†ä¿å­˜åœ¨æ­¤ç›®å½•")
        print("="*60)
    
    # æ³¨æ„ï¼šsearch_by_keyword æ–¹æ³•åº”è¯¥ä¸ __init__ æ–¹æ³•åŒçº§ï¼Œä¸æ˜¯å†…éƒ¨æ–¹æ³•
    def search_by_keyword(self, keyword, search_field="title", days_limit=10, site_id=None, category_id=None, referer_url=None):
        """æŒ‰å…³é”®è¯æœç´¢ç‰¹å®šç½‘ç«™"""
        all_data = []
        page_no = 1
        
        # ä½¿ç”¨å‚æ•°æˆ–é»˜è®¤å€¼
        site_id = site_id or self.default_site_id
        category_id = category_id or self.default_category_id
        
        # è®¾ç½®Refererå¤´
        headers = self.headers.copy()
        if referer_url:
            headers['Referer'] = f"{self.base_url}{referer_url}"
        else:
            headers['Referer'] = f"{self.base_url}/cms/default/webfile/3ywgg1/index.html"
        
        # è®¡ç®—æ—¥æœŸèŒƒå›´
        end_date = datetime.now()
        start_date = end_date - timedelta(days=days_limit)
        
        while True:
            try:
                payload = {
                    "pageNo": page_no,
                    "pageSize": self.page_size,
                    "dto": {
                        "siteId": site_id,
                        "categoryId": category_id,
                        "beginDate": start_date.strftime("%Y-%m-%d"),
                        "endDate": end_date.strftime("%Y-%m-%d"),
                    }
                }
                
                if search_field == "title":
                    payload["dto"]["title"] = keyword
                elif search_field == "agentCompanyName":
                    payload["dto"]["agentCompanyName"] = keyword
                
                # æ„å»ºåŸºç¡€è¯·æ±‚å‚æ•°
                request_params = {
                    'url': self.api_url,
                    'headers': headers,
                    'data': json.dumps(payload, ensure_ascii=False).encode('utf-8'),
                    'timeout': 30
                }
                
                # ä»…å½“éœ€è¦ä»£ç†æ—¶ï¼Œæ‰æ·»åŠ  proxies å‚æ•°
                if self.use_proxy:
                    request_params['proxies'] = self.proxy_config
                    if page_no == 1:
                        print(f"ğŸ“¡ ä½¿ç”¨ä»£ç†è¯·æ±‚: {self.proxy_config['http']}")
                
                response = requests.post(**request_params)
                
                if response.status_code != 200:
                    logger.error(f"HTTP {response.status_code}: è¯·æ±‚å¤±è´¥")
                    print(f"âŒ è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    break
                
                data = response.json()
                rows = data['res'].get('rows', [])
                total = data['res'].get('total', 0)
                
                if page_no == 1:
                    logger.info(f"ç½‘ç«™é…ç½®[site_id={site_id}, category_id={category_id}] - æ€»å…±æ‰¾åˆ° {total} æ¡ç›¸å…³è®°å½•")
                    print(f"âœ… è¯·æ±‚æˆåŠŸï¼Œæ‰¾åˆ° {total} æ¡ç›¸å…³è®°å½•")
                
                if not rows:
                    break
                
                all_data.extend(rows)
                
                if len(rows) < self.page_size:
                    break
                    
                page_no += 1
                time.sleep(1)  # å¢åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡å¿«
                
            except requests.exceptions.ProxyError as e:
                logger.error(f"ä»£ç†è¿æ¥å¤±è´¥: {e}")
                print(f"âŒ ä»£ç†è¿æ¥å¤±è´¥: {e}")
                print("å°è¯•ä½¿ç”¨å¤‡ç”¨ä»£ç†æˆ–ç›´æ¥è¿æ¥...")
                break
            except requests.exceptions.ConnectionError as e:
                logger.error(f"è¿æ¥é”™è¯¯: {e}")
                print(f"âŒ è¿æ¥é”™è¯¯: {e}")
                break
            except Exception as e:
                logger.error(f"æœç´¢å¼‚å¸¸: {e}")
                print(f"âŒ æœç´¢å¼‚å¸¸: {e}")
                break
        
        return all_data
    
    def search_website(self, website_config, days_limit=10):
        """æœç´¢å•ä¸ªç½‘ç«™çš„æ‰€æœ‰å…³é”®è¯"""
        website_results = []
        website_name = website_config["name"]
        
        logger.info(f"\n{'='*60}")
        logger.info(f"å¼€å§‹çˆ¬å–ç½‘ç«™: {website_name}")
        logger.info(f"ç½‘ç«™URL: {website_config['url']}")
        # ä¿®å¤ï¼šåˆ é™¤å¤šä½™çš„ logger. å’Œé‡å¤çš„å˜é‡å
        logger.info(f"é…ç½®: site_id={website_config['site_id']}, category_id={website_config['category_id']}")
        
        for keyword in self.keywords:
            logger.info(f"å¤„ç†å…³é”®è¯: {keyword}")
            
            # åœ¨æ ‡é¢˜ä¸­æœç´¢
            title_results = self.search_by_keyword(
                keyword, "title", days_limit,
                website_config["site_id"], website_config["category_id"],
                website_config["url"]
            )
            time.sleep(1)
            
            # åœ¨é‡‡è´­å•ä½ä¸­æœç´¢
            company_results = self.search_by_keyword(
                keyword, "agentCompanyName", days_limit,
                website_config["site_id"], website_config["category_id"],
                website_config["url"]
            )
            
            # åˆå¹¶ç»“æœ
            keyword_results = title_results + company_results
            
            # å»é‡
            seen = set()
            unique_results = []
            for item in keyword_results:
                item_id = f"{item.get('title', '')}_{item.get('publishDate', '')}"
                if item_id not in seen:
                    seen.add(item_id)
                    unique_results.append(item)
            
            logger.info(f"å…³é”®è¯ '{keyword}' åœ¨ç½‘ç«™ '{website_name}' å»é‡åå¾—åˆ° {len(unique_results)} æ¡å”¯ä¸€æ•°æ®")
            
            # æå–æ•°æ®
            for item in unique_results:
                extracted = self.extract_item_fields(item)
                extracted['æœç´¢å…³é”®è¯'] = keyword
                extracted['æ¥æºç½‘ç«™'] = website_name
                extracted['ç½‘ç«™URL'] = f"{self.base_url}{website_config['url']}"
                website_results.append(extracted)
        
        logger.info(f"ç½‘ç«™ '{website_name}' æ€»è®¡çˆ¬å– {len(website_results)} æ¡æ•°æ®")
        return website_results
    
    # ä¿æŒä¸æ—§ä»£ç å…¼å®¹çš„æ–¹æ³•
    def search_all_keywords(self, days_limit=10):
        """å…¼å®¹æ—§ç‰ˆæœ¬çš„æœç´¢æ–¹æ³•ï¼ˆåªæœç´¢ç¬¬ä¸€ä¸ªç½‘ç«™ï¼‰"""
        logger.info("ä½¿ç”¨å…¼å®¹æ¨¡å¼ï¼šåªæœç´¢ç¬¬ä¸€ä¸ªç½‘ç«™")
        website_data = self.search_website(self.website_configs[0], days_limit)
        
        # ç§»é™¤ç½‘ç«™ç›¸å…³å­—æ®µä»¥ä¿æŒä¸æ—§ç‰ˆæœ¬çš„å…¼å®¹æ€§
        clean_data = []
        for item in website_data:
            clean_item = item.copy()
            if 'æ¥æºç½‘ç«™' in clean_item:
                del clean_item['æ¥æºç½‘ç«™']
            if 'ç½‘ç«™URL' in clean_item:
                del clean_item['ç½‘ç«™URL']
            clean_data.append(clean_item)
                
        return clean_data
    
    def search_all_websites(self, days_limit=10):
        """æœç´¢æ‰€æœ‰ç½‘ç«™çš„å…³é”®è¯ï¼ˆæ–°æ–¹æ³•ï¼‰"""
        all_results = []
        
        print(f"\n{'='*60}")
        print("ğŸš€ å¼€å§‹çˆ¬å–æ‰€æœ‰ç½‘ç«™")
        print(f"æœç´¢å…³é”®è¯: {self.keywords}")
        print(f"æ—¶é—´èŒƒå›´: æœ€è¿‘{days_limit}å¤©")
        print(f"ç½‘ç«™æ•°é‡: {len(self.website_configs)}ä¸ª")
        if self.use_proxy:
            print(f"ğŸ“¡ ä½¿ç”¨ä»£ç†: {self.proxy_config['http']}")
        print(f"{'='*60}\n")
        
        for config in self.website_configs:
            try:
                print(f"ğŸŒ æ­£åœ¨çˆ¬å–ç½‘ç«™: {config['name']}")
                
                # çˆ¬å–å½“å‰ç½‘ç«™
                website_data = self.search_website(config, days_limit)
                
                # ç§»é™¤ç½‘ç«™ç›¸å…³å­—æ®µ
                clean_data = []
                for item in website_data:
                    # åˆ›å»ºå‰¯æœ¬ï¼Œé¿å…ä¿®æ”¹åŸæ•°æ®
                    clean_item = item.copy()
                    if 'æ¥æºç½‘ç«™' in clean_item:
                        del clean_item['æ¥æºç½‘ç«™']
                    if 'ç½‘ç«™URL' in clean_item:
                        del clean_item['ç½‘ç«™URL']
                    clean_data.append(clean_item)
                
                print(f"âœ… ç½‘ç«™ '{config['name']}' çˆ¬å–å®Œæˆ: {len(clean_data)} æ¡æ•°æ®")
                all_results.extend(clean_data)
                
                # ç½‘ç«™é—´å»¶è¿Ÿ
                if config != self.website_configs[-1]:  # ä¸æ˜¯æœ€åä¸€ä¸ªç½‘ç«™
                    time.sleep(2)
                
            except Exception as e:
                print(f"âŒ çˆ¬å–ç½‘ç«™ {config['name']} æ—¶å‡ºé”™: {e}")
                continue
        
        # è·¨ç½‘ç«™å»é‡
        if all_results:
            seen = set()
            unique_results = []
            for item in all_results:
                # ä½¿ç”¨æ ‡é¢˜+å‘å¸ƒæ—¶é—´ä½œä¸ºå”¯ä¸€æ ‡è¯†
                item_id = f"{item.get('æ ‡é¢˜', '')}_{item.get('å‘å¸ƒæ—¶é—´', '')}"
                if item_id not in seen:
                    seen.add(item_id)
                    unique_results.append(item)
            
            print(f"\nğŸ“Š æ‰€æœ‰ç½‘ç«™çˆ¬å–å®Œæˆ")
            print(f"åŸå§‹æ•°æ®: {len(all_results)} æ¡")
            print(f"å»é‡å: {len(unique_results)} æ¡")
            print(f"{'='*60}")
        
        return unique_results if all_results else []
    
    def extract_item_fields(self, item):
        """æå–æ•°æ®å­—æ®µ"""
        publish_date = item.get('publishDate', '')
        if publish_date and 'T' in publish_date:
            publish_date = publish_date.split('T')[0]
        
        def format_date(date_str):
            if date_str and 'T' in date_str:
                return date_str.split('T')[0]
            return date_str or ''
        
        return {
            'æ ‡é¢˜': item.get('title', ''),
            'å‘å¸ƒæ—¶é—´': publish_date,
            'é‡‡è´­å•ä½': item.get('agentCompanyName', ''),
            'é¡¹ç›®ç¼–å·': item.get('mainCode', ''),
            'é‡‡è´­æ–¹å¼': item.get('purchaseModeName', item.get('purchaseMode', '')),
            'çœä»½': item.get('provinceName', ''),
            'åŸå¸‚': item.get('cityName', ''),
            'åˆ†ç±»': item.get('categoryName', ''),
            'é“¾æ¥': f"{self.base_url}{item.get('url', '')}" if item.get('url') else '',
            'è¯¦ç»†å†…å®¹': (item.get('text', '')[:100] + '...') if item.get('text') else '',
        }
    
    def save_results(self, data):
        """ä¿å­˜ç»“æœ"""
        if not data:
            print("âš ï¸  æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        df = pd.DataFrame(data)
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"æ™‹èƒ½æ§è‚¡æ‹›æ ‡_{timestamp}"
        
        try:
            # ä¿å­˜Excel
            excel_file = f"{filename}.xlsx"
            df.to_excel(excel_file, index=False, engine='openpyxl')
            print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°Excel:")
            print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {os.path.abspath(excel_file)}")
            
            # ä¿å­˜CSV
            csv_file = f"{filename}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {os.path.abspath(csv_file)}")
            
        except Exception as e:
            print(f"ä¿å­˜Excelå¤±è´¥: {e}")
            # åªä¿å­˜CSV
            csv_file = f"{filename}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°CSV: {os.path.abspath(csv_file)}")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"æ€»è®¡æ•°æ®: {len(df)} æ¡")
        print(f"æ—¶é—´èŒƒå›´: {df['å‘å¸ƒæ—¶é—´'].min()} è‡³ {df['å‘å¸ƒæ—¶é—´'].max()}")
        
        return df
    
    def run(self):
        """è¿è¡Œçˆ¬è™«ï¼ˆå¤šç½‘ç«™ç‰ˆï¼‰"""
        print("ğŸš€ å¯åŠ¨æ™‹èƒ½æ§è‚¡æ‹›æ ‡æ•°æ®çˆ¬è™«ï¼ˆå¤šç½‘ç«™ç‰ˆï¼‰")
        print(f"æœç´¢å…³é”®è¯: {self.keywords}")
        print(f"çˆ¬å–ç½‘ç«™æ•°: {len(self.website_configs)} ä¸ª")
        print(f"æ—¶é—´èŒƒå›´: æœ€è¿‘10å¤©")
        
        all_data = []
        
        for config in self.website_configs:
            try:
                # å¦‚æœé…ç½®ä¸­æ²¡æœ‰site_idæˆ–category_idï¼Œå°è¯•å‘ç°
                if "site_id" not in config or "category_id" not in config:
                    logger.warning(f"ç½‘ç«™ {config['name']} ç¼ºå°‘é…ç½®å‚æ•°ï¼Œå°è¯•ä½¿ç”¨é»˜è®¤å€¼")
                    config["site_id"] = self.default_site_id
                    config["category_id"] = self.default_category_id
                
                # çˆ¬å–å½“å‰ç½‘ç«™
                website_data = self.search_website(config, days_limit=10)
                all_data.extend(website_data)
                
                # ç½‘ç«™é—´å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(2)
                
            except Exception as e:
                logger.error(f"çˆ¬å–ç½‘ç«™ {config['name']} æ—¶å‡ºé”™: {e}")
                continue
        
        if not all_data:
            print("âš ï¸  æ‰€æœ‰ç½‘ç«™å‡æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„æ•°æ®")
            return
        
        # æœ€ç»ˆå»é‡ï¼ˆè·¨ç½‘ç«™å»é‡ï¼‰
        seen = set()
        unique_data = []
        for item in all_data:
            # ä½¿ç”¨æ ‡é¢˜+å‘å¸ƒæ—¶é—´+æ¥æºç½‘ç«™ä½œä¸ºå”¯ä¸€æ ‡è¯†
            item_id = f"{item.get('æ ‡é¢˜', '')}_{item.get('å‘å¸ƒæ—¶é—´', '')}_{item.get('æ¥æºç½‘ç«™', '')}"
            if item_id not in seen:
                seen.add(item_id)
                unique_data.append(item)
        
        logger.info(f"è·¨ç½‘ç«™å»é‡åæ€»è®¡ {len(unique_data)} æ¡å”¯ä¸€æ•°æ®")
        
        # ä¿å­˜ç»“æœ
        if 'æ¥æºç½‘ç«™' in unique_data[0]:
            # å¦‚æœæœ‰æ¥æºç½‘ç«™å­—æ®µï¼Œä½¿ç”¨å¢å¼ºç‰ˆä¿å­˜
            self.save_results_enhanced(unique_data)
        else:
            # å¦åˆ™ä½¿ç”¨æ™®é€šä¿å­˜
            self.save_results(unique_data)
        
        print(f"\nğŸ‰ çˆ¬è™«æ‰§è¡Œå®Œæˆï¼")
    
    def save_results_enhanced(self, data):
        """å¢å¼ºç‰ˆä¿å­˜ç»“æœï¼ˆåŒ…å«å¤šç½‘ç«™ä¿¡æ¯ï¼‰"""
        if not data:
            print("âš ï¸  æ²¡æœ‰æ•°æ®å¯ä¿å­˜")
            return None
        
        df = pd.DataFrame(data)
        
        # æŒ‰æ¥æºç½‘ç«™åˆ†ç»„ç»Ÿè®¡
        website_stats = df['æ¥æºç½‘ç«™'].value_counts()
        
        # ç”Ÿæˆæ–‡ä»¶å
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"æ™‹èƒ½æ§è‚¡æ‹›æ ‡_å¤šç½‘ç«™_{timestamp}"
        
        try:
            # ä¿å­˜Excel
            excel_file = f"{filename}.xlsx"
            
            # ä½¿ç”¨ExcelWriteråˆ›å»ºå¤šä¸ªsheet
            with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
                # ä¸»è¡¨ï¼šæ‰€æœ‰æ•°æ®
                df.to_excel(writer, sheet_name='æ‰€æœ‰æ•°æ®', index=False)
                
                # æŒ‰ç½‘ç«™åˆ†è¡¨
                for website in df['æ¥æºç½‘ç«™'].unique():
                    website_df = df[df['æ¥æºç½‘ç«™'] == website]
                    # ç®€åŒ–sheetåï¼ˆExcel sheetåæœ€å¤š31å­—ç¬¦ï¼‰
                    sheet_name = f"{website}"[:31]
                    website_df.to_excel(writer, sheet_name=sheet_name, index=False)
                
                # ç»Ÿè®¡æ•°æ®è¡¨
                stats_df = pd.DataFrame({
                    'ç½‘ç«™': website_stats.index,
                    'æ•°æ®é‡': website_stats.values
                })
                stats_df.to_excel(writer, sheet_name='ç»Ÿè®¡', index=False)
            
            print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°Excel:")
            print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {os.path.abspath(excel_file)}")
            
            # ä¿å­˜CSV
            csv_file = f"{filename}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"ğŸ“ æ–‡ä»¶ä½ç½®: {os.path.abspath(csv_file)}")
            
        except Exception as e:
            print(f"ä¿å­˜Excelå¤±è´¥: {e}")
            # åªä¿å­˜CSV
            csv_file = f"{filename}.csv"
            df.to_csv(csv_file, index=False, encoding='utf-8-sig')
            print(f"âœ… æ•°æ®å·²ä¿å­˜åˆ°CSV: {os.path.abspath(csv_file)}")
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"æ€»è®¡æ•°æ®: {len(df)} æ¡")
        for website, count in website_stats.items():
            print(f"  - {website}: {count} æ¡")
        print(f"æ—¶é—´èŒƒå›´: {df['å‘å¸ƒæ—¶é—´'].min()} è‡³ {df['å‘å¸ƒæ—¶é—´'].max()}")

def main():
    spider = JnkgBiddingSpider()
    spider.run()

if __name__ == "__main__":
    main()
